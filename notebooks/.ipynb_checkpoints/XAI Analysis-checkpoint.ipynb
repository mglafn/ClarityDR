{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4034ae50",
   "metadata": {},
   "source": [
    "# Step 4: XAI Analysis and Interpretation\n",
    "\n",
    "This notebook implements **Sections 3.3 and 3.4** of the research proposal. Having trained Model A (Transfer Learning) and Model B (Fine-Tuning), we will now:\n",
    "\n",
    "1.  Load the best-performing checkpoints for both models.\n",
    "2.  Run inference on the entire validation set to identify specific examples of **True Positives, False Positives, and False Negatives**.\n",
    "3.  Set up the implementation for our three XAI methods: Grad-CAM, SHAP, and Integrated Gradients.\n",
    "4.  Generate and visualize side-by-side comparisons of the explanations for our curated examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41637974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Add our custom source code to the path\n",
    "sys.path.append(\"../src\")\n",
    "# Import our data setup and model class\n",
    "from data_setup import create_dataloaders\n",
    "from model import DRClassifier \n",
    "\n",
    "# --- Configuration ---\n",
    "RAW_DATA_DIR = '../data/raw/aptos2019-blindness-detection/'\n",
    "PROCESSED_DATA_DIR = '../data/processed/'\n",
    "IMAGE_DIR = os.path.join(RAW_DATA_DIR, 'train_images/')\n",
    "MODEL_A_PATH = '../models/model_a_best.ckpt'\n",
    "MODEL_B_PATH = '../models/model_b_best.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bd6821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load the Trained Models ---\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load Model A (Transfer Learning)\n",
    "model_a = DRClassifier.load_from_checkpoint(MODEL_A_PATH).to(device)\n",
    "model_a.eval() # Set to evaluation mode\n",
    "\n",
    "# Load Model B (Fine-Tuning)\n",
    "model_b = DRClassifier.load_from_checkpoint(MODEL_B_PATH).to(device)\n",
    "model_b.eval() # Set to evaluation mode\n",
    "\n",
    "print(\"Models loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a83e091",
   "metadata": {},
   "source": [
    "### 4.1: Generating and Caching Model Predictions\n",
    "\n",
    "To create an efficient workflow, we will run inference on the entire validation set once for each model and then **cache the results** in a CSV file. On subsequent runs, if these files exist, we'll load them directly instead of re-running the time-consuming prediction loop.\n",
    "\n",
    "This allows us to quickly experiment with the XAI analysis without waiting for inference every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1097f486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define paths for cached predictions ---\n",
    "results_a_path = os.path.join(PROCESSED_DATA_DIR, 'results_model_a.csv')\n",
    "results_b_path = os.path.join(PROCESSED_DATA_DIR, 'results_model_b.csv')\n",
    "\n",
    "# --- Check if cached results exist ---\n",
    "if os.path.exists(results_a_path) and os.path.exists(results_b_path):\n",
    "    print(\"Loading cached predictions...\")\n",
    "    results_a_df = pd.read_csv(results_a_path)\n",
    "    results_b_df = pd.read_csv(results_b_path)\n",
    "\n",
    "else:\n",
    "    print(\"Cached predictions not found. Generating and caching new predictions...\")\n",
    "    \n",
    "    import time\n",
    "\n",
    "    # MODIFIED FUNCTION SIGNATURE: Added 'dataframe' parameter\n",
    "    def get_predictions(model, loader, device, dataframe):\n",
    "        \"\"\"Run inference on the dataloader and return results with debug prints.\"\"\"\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        # We zip the dataloader with the dataframe. Because batch_size=1 and shuffle=False,\n",
    "        # they will match up perfectly, row by row.\n",
    "        # enumerate gives us a counter (batch_idx) for our debug prints.\n",
    "        data_iterator = tqdm(zip(loader, dataframe.iterrows()), total=len(dataframe), desc=\"Getting Predictions\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, ((images, labels), (_, row)) in enumerate(data_iterator):\n",
    "                \n",
    "                # --- NEW DEBUGGING BLOCK ---\n",
    "                # Print an update every 50 images\n",
    "                if batch_idx % 50 == 0:\n",
    "                    image_id = row['id_code']\n",
    "                    # This print will appear above the tqdm progress bar\n",
    "                    print(f\"  [Debug] Processing image {batch_idx}/{len(dataframe)}: {image_id}.png\")\n",
    "                # --- END NEW DEBUGGING BLOCK ---\n",
    "\n",
    "                images = images.to(device)\n",
    "                logits = model(images)\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                \n",
    "                time.sleep(0.05)\n",
    "\n",
    "        return np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "    # Recreate the validation dataloader\n",
    "    val_df = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, 'val_split.csv'))\n",
    "    _, val_loader_inference = create_dataloaders(\n",
    "        train_df=val_df, \n",
    "        val_df=val_df, \n",
    "        image_dir=IMAGE_DIR, \n",
    "        batch_size=1, \n",
    "        num_workers=0 # This remains 0, which is critical\n",
    "    )\n",
    "\n",
    "    # MODIFIED FUNCTION CALLS: Pass the val_df dataframe\n",
    "    print(\"\\n--- Generating predictions for Model A ---\")\n",
    "    preds_a, labels_a = get_predictions(model_a, val_loader_inference, device, val_df)\n",
    "    \n",
    "    print(\"\\n--- Generating predictions for Model B ---\")\n",
    "    preds_b, labels_b = get_predictions(model_b, val_loader_inference, device, val_df)\n",
    "\n",
    "    # Create results dataframes\n",
    "    results_a_df = val_df.copy()\n",
    "    results_a_df['predicted_label'] = preds_a\n",
    "    results_a_df['true_label'] = labels_a\n",
    "\n",
    "    results_b_df = val_df.copy()\n",
    "    results_b_df['predicted_label'] = preds_b\n",
    "    results_b_df['true_label'] = labels_b\n",
    "\n",
    "    # Cache the results\n",
    "    results_a_df.to_csv(results_a_path, index=False)\n",
    "    results_b_df.to_csv(results_b_path, index=False)\n",
    "    print(\"Predictions cached successfully.\")\n",
    "\n",
    "# --- Identify cases for analysis (This part is unchanged) ---\n",
    "fp_a = results_a_df[(results_a_df['true_label'] == 0) & (results_a_df['predicted_label'] > 0)]\n",
    "fn_a = results_a_df[(results_a_df['true_label'] > 0) & (results_a_df['predicted_label'] == 0)]\n",
    "tp_a = results_a_df[(results_a_df['true_label'] > 0) & (results_a_df['predicted_label'] > 0) & (results_a_df['true_label'] == results_a_df['predicted_label'])]\n",
    "\n",
    "print(\"\\n--- Analysis Cases for Model A ---\")\n",
    "print(f\"Found {len(tp_a)} True Positives (diseased)\")\n",
    "print(f\"Found {len(fp_a)} False Positives\")\n",
    "print(f\"Found {len(fn_a)} False Negatives\")\n",
    "\n",
    "print(\"\\nExample of a False Negative from Model A:\")\n",
    "print(fn_a.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b74768-d4e8-4ef8-9de6-61cad4b95a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Identify cases for analysis from the (now loaded) dataframes ---\n",
    "fp_a = results_a_df[(results_a_df['true_label'] == 0) & (results_a_df['predicted_label'] > 0)]\n",
    "fn_a = results_a_df[(results_a_df['true_label'] > 0) & (results_a_df['predicted_label'] == 0)]\n",
    "tp_a = results_a_df[(results_a_df['true_label'] > 0) & (results_a_df['predicted_label'] > 0) & (results_a_df['true_label'] == results_a_df['predicted_label'])]\n",
    "\n",
    "print(\"\\n--- Analysis Cases for Model A ---\")\n",
    "print(f\"Found {len(tp_a)} True Positives (diseased)\")\n",
    "print(f\"Found {len(fp_a)} False Positives\")\n",
    "print(f\"Found {len(fn_a)} False Negatives\")\n",
    "\n",
    "print(\"\\nExample of a False Negative from Model A:\")\n",
    "print(fn_a.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f862dc-975d-4d0a-bd4c-b9c99443a96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133eef0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
