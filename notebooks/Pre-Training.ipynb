{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48784c79",
   "metadata": {},
   "source": [
    "# Step 1: Exploratory Data Analysis (EDA)\n",
    "\n",
    "This initial phase focuses on understanding the APTOS 2019 dataset. We will:\n",
    "1. Load the training labels.\n",
    "2. Visualize a sample image from each of the five diagnostic classes.\n",
    "3. Plot the distribution of the classes to check for imbalance.\n",
    "\n",
    "This exploration is critical as it informs our preprocessing strategy. The significant class imbalance observed will necessitate a **stratified** train-validation split to ensure the model sees a representative distribution of classes during both training and evaluation. Dataset from [Kaggle](https://www.kaggle.com/c/aptos2019-blindness-detection/overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2645e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "import os\n",
    "\n",
    "gitlab_token = userdata.get('GITLAB_TOKEN')\n",
    "\n",
    "repository_url = f\"https://oauth2:{gitlab_token}@gitlab.com/mglafn/ClarityDR.git\"\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "destination_path = \"/content/drive/MyDrive/Colab Notebooks/ClarityDR\" \n",
    "os.makedirs(destination_path, exist_ok=True)\n",
    "\n",
    "!git clone {repository_url} \"{destination_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af11109",
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_path = \"/content/drive/MyDrive/Colab Notebooks/ClarityDR\" \n",
    "os.makedirs(destination_path, exist_ok=True)\n",
    "\n",
    "!git config --global user.name \"mglafn\"\n",
    "!git config --global user.email \"mglafn@icloud.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d3e402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Add our custom source code to the path\n",
    "sys.path.append(\"../src\")  \n",
    "# Import our custom functions\n",
    "from data_setup import get_data_splits, create_dataloaders\n",
    "\n",
    "# Load the main dataset labels\n",
    "df_train = pd.read_csv('../data/raw/aptos2019-blindness-detection/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364cc0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_dir = '../data/raw/aptos2019-blindness-detection/train_images/'\n",
    "\n",
    "class_labels = {\n",
    "    0: 'No DR',\n",
    "    1: 'Mild',\n",
    "    2: 'Moderate',\n",
    "    3: 'Severe',\n",
    "    4: 'Proliferative'\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "for i in range(5):\n",
    "    # Find the first image for the current class\n",
    "    sample_id = df_train[df_train['diagnosis'] == i].iloc[0]['id_code']\n",
    "    image_path = os.path.join(train_image_dir, f\"{sample_id}.png\")\n",
    "    \n",
    "    # Load the image\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Convert for matplotlib\n",
    "    img = cv2.resize(img, (224, 224)) # Resize for consistent display\n",
    "    \n",
    "    # Display the image\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Class: {i} ({class_labels[i]})\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2935aa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = df_train['diagnosis'].value_counts().sort_index()\n",
    "\n",
    "# Plot the distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.barplot(x=class_counts.index, y=class_counts.values, palette=\"viridis\", hue=class_counts.index, legend=False)\n",
    "\n",
    "plt.title('Distribution of Diabetic Retinopathy Classes')\n",
    "plt.xlabel('Diagnosis (Severity)')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.xticks(ticks=[0, 1, 2, 3, 4], labels=['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferative'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ac6a13",
   "metadata": {},
   "source": [
    "# Step 2: Data Preparation\n",
    "\n",
    "This section implements **Section 3.1 (Dataset and Preprocessing)** of the research proposal. We delegate the logic to our `data_setup.py` module to keep this notebook clean.\n",
    "\n",
    "### 2.1: Splitting and Caching the Data\n",
    "\n",
    "We first split the data into stratified 80/20 training and validation sets. To save time on future runs, our custom `get_data_splits` function will automatically save the resulting dataframes to `train_split.csv` and `val_split.csv` in the `../data/processed` directory. If these files already exist, the function will load them directly, skipping the split process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ed5e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for our data\n",
    "raw_data_dir = '../data/raw/aptos2019-blindness-detection/'\n",
    "processed_data_dir = '../data/processed/'\n",
    "train_image_dir = os.path.join(raw_data_dir, 'train_images/')\n",
    "\n",
    "# Create the processed data directory if it doesn't exist\n",
    "os.makedirs(processed_data_dir, exist_ok=True)\n",
    "\n",
    "# Get the train/validation splits (from cache or by creating them)\n",
    "train_df, val_df = get_data_splits(df_train, processed_data_dir)\n",
    "\n",
    "print(f\"\\nTraining set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(\"\\nValidation set distribution:\\n\", val_df['diagnosis'].value_counts(normalize=True).sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c8895b",
   "metadata": {},
   "source": [
    "### 2.2: Creating the DataLoaders\n",
    "\n",
    "Now that we have our dataframes, we use our `create_dataloaders` helper function. This function handles all the PyTorch-specific boilerplate:\n",
    "-   Applying the correct transformations and data augmentation.\n",
    "-   Wrapping the dataframes in a custom `Dataset` class.\n",
    "-   Setting up the `DataLoader` to efficiently serve batches of data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb5b69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create the DataLoaders ---\n",
    "BATCH_SIZE = 32\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    train_df=train_df,\n",
    "    val_df=val_df,\n",
    "    image_dir=train_image_dir,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers= 0\n",
    ")\n",
    "\n",
    "# Verify that the dataloader is working correctly\n",
    "images, labels = next(iter(train_loader))\n",
    "print(f\"\\nShape of a batch of images: {images.shape}\")\n",
    "print(f\"Shape of a batch of labels: {labels.shape}\")\n",
    "print(f\"Data type of images: {images.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d86c0f5",
   "metadata": {},
   "source": [
    "# Step 3: Model Architecture and Training\n",
    "\n",
    "This section implements **Section 3.2** of the research proposal. We will:\n",
    "1.  Define a `DRClassifier` class using PyTorch Lightning. This class encapsulates the ResNet50 model, the training logic, validation logic, and optimizer configuration.\n",
    "2.  Instantiate and train **Model A (Transfer Learning)**, where only the final classification layer is trained.\n",
    "3.  Instantiate and train **Model B (Fine-Tuning)**, where all layers are unfrozen and trained with a low learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75ccfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torchmetrics import Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba4ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRClassifier(pl.LightningModule):\n",
    "    def __init__(self, num_classes=5, learning_rate=1e-3, unfreeze_base=False):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() # Saves args to self.hparams\n",
    "\n",
    "        # Load the pretrained ResNet50 model\n",
    "        self.model = models.resnet50(weights='IMAGENET1K_V2')\n",
    "\n",
    "        # Freeze the convolutional base if specified (for Model A)\n",
    "        if not unfreeze_base:\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # Replace the final fully connected layer\n",
    "        num_ftrs = self.model.fc.in_features\n",
    "        self.model.fc = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "        # Define loss and metrics\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        acc = self.accuracy(logits, y)\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        acc = self.accuracy(logits, y)\n",
    "\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', acc, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # The optimizer will only update the parameters that have requires_grad=True\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b68240",
   "metadata": {},
   "source": [
    "### 3.1: Training Model A (Transfer Learning)\n",
    "\n",
    "Here, we train the first model where the ResNet50 base is frozen. We are only training the weights of the new classification head. We'll use a `ModelCheckpoint` callback to automatically save the version of the model that achieves the best validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daec3655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration for Model A ---\n",
    "LEARNING_RATE_A = 1e-3\n",
    "MODEL_A_PATH = '../models/model_a_transfer_learning.ckpt'\n",
    "\n",
    "# --- Setup Model A ---\n",
    "model_a = DRClassifier(\n",
    "    num_classes=5,\n",
    "    learning_rate=LEARNING_RATE_A,\n",
    "    unfreeze_base=False \n",
    ")\n",
    "\n",
    "# --- Setup Callbacks ---\n",
    "checkpoint_callback_a = ModelCheckpoint(\n",
    "    monitor='val_acc',\n",
    "    dirpath='../models/',\n",
    "    filename='model_a_best',\n",
    "    save_top_k=1,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "# --- Setup Trainer for Model A ---\n",
    "trainer_a = pl.Trainer(\n",
    "    max_epochs=10, \n",
    "    accelerator='auto', # Automatically uses GPU/MPS if available\n",
    "    callbacks=[checkpoint_callback_a],\n",
    "    logger=pl.loggers.TensorBoardLogger('../logs/', name='model_a')\n",
    ")\n",
    "\n",
    "# --- Start Training ---\n",
    "trainer_a.fit(model_a, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b711f53",
   "metadata": {},
   "source": [
    "### 3.2: Training Model B (Fine-Tuning)\n",
    "\n",
    "Now, we train the second model where the entire network is unfrozen. As specified in the proposal, we use a much lower learning rate to prevent the pre-trained weights from being corrupted by large gradient updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f19c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration for Model B ---\n",
    "LEARNING_RATE_B = 1e-5 # Lower learning rate for fine-tuning\n",
    "MODEL_B_PATH = '../models/model_b_fine_tuned.ckpt'\n",
    "\n",
    "# --- Setup Model B ---\n",
    "model_b = DRClassifier(\n",
    "    num_classes=5,\n",
    "    learning_rate=LEARNING_RATE_B,\n",
    "    unfreeze_base=True # Unfreeze the entire network\n",
    ")\n",
    "\n",
    "# --- Setup Callbacks ---\n",
    "checkpoint_callback_b = ModelCheckpoint(\n",
    "    monitor='val_acc',\n",
    "    dirpath='../models/',\n",
    "    filename='model_b_best',\n",
    "    save_top_k=1,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "# --- Setup Trainer for Model B ---\n",
    "trainer_b = pl.Trainer(\n",
    "    max_epochs=15, # May need more epochs for fine-tuning\n",
    "    accelerator='auto',\n",
    "    callbacks=[checkpoint_callback_b],\n",
    "    logger=pl.loggers.TensorBoardLogger('../logs/', name='model_b')\n",
    ")\n",
    "\n",
    "# --- Start Training ---\n",
    "trainer_b.fit(model_b, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-analysis-mac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
